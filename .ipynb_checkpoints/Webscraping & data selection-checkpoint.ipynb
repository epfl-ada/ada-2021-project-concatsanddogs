{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9551c5c5",
   "metadata": {},
   "source": [
    "## Header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f485ebf",
   "metadata": {},
   "source": [
    "This notebook is organized followings the different steps used in our pipeline. We first create a list of keywords using [web scraping](#Webscraping) and a personal list of keywords. \n",
    "With this list of keywords we [select](#Dataset-selection-from-Quotebank-database) a subset of the Quotebank database. This subset will be our starting dataset for our project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f2be5c",
   "metadata": {},
   "source": [
    "## General librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceb1f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b9c72",
   "metadata": {},
   "source": [
    "## Webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6339b135",
   "metadata": {},
   "source": [
    "We decided to scrape the **usnews.com** website because they have some topic pages that list all articles on the specified topic.   \n",
    "For example this page https://www.usnews.com/topics/subjects/feminism identifies all the articles from usnews.com that are relevant on the topic of feminism in *the latest* column. Further down we will call these topic pages, primary URLs. We then access all the identified articles on their urls, i.e. secondary urls, and retrieve their contents to create a corpus of text relevant to our topic. The corpus is saved in *Articles_Contents.txt.*   \n",
    "The corpus is used to retrieve bigrams. We decided to not count onegrame because they are too general for our purpose, for example 'women' gives a lot of results but isn't always of interest. The following quote \"a woman, a woman, a woman.\" from an unknown speaker isn't relevant for our purpose.\n",
    "The bigrams complete a manual list of keywords that are used to select our quotes of interest. The web-scraped keywords are necessary to ensure that we don't miss frequent bigrams and to remove some of the bias that exist in a personal keywords list.\n",
    "Note that at this point we used only usnews as a source and we were not able to mimick the infinite scrolling so only a limited list of articles per topic is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ddd59a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a788458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aminamatt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import requests #http library\n",
    "import nltk #natural language processing library\n",
    "nltk.download('stopwords') #common english words to ignore \n",
    "from bs4 import BeautifulSoup #extraction from HTML and XML files\n",
    "from collections import Counter #dictionary subclass for counting hashable objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f0265e",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0ee61547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_usnews(URL):\n",
    "    '''\n",
    "    Description: Retrieving of urls of articles from a topic page of usnews.com\n",
    "    Input: the primary URL string, i.e. the URL with the list of relevant articles\n",
    "    Output: a list of urls strings referring to relevant articles \n",
    "    Requirements : Request, BeautifulSoup libraries\n",
    "    Use: This function is made to be used to scrap the to usnews.com website. \n",
    "    If you want to adapt it to another website the class tag should be adapted.\n",
    "    '''\n",
    "    \n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}\n",
    "    response = requests.get(URL,headers=headers) #http request with a user-agent string to avoid blocking from server\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') #parse the document with html format\n",
    "    latest = soup.find('div',{'class':\"LoadMoreWrapper__Container-zwyk5c-0 himujt\"}) #get all the elements within 'the latest'category\n",
    "   \n",
    "\n",
    "    #Find all the urls in the articles of latest category\n",
    "    list_of_urls = []\n",
    "    try :\n",
    "        for a in latest.find_all('a'):\n",
    "            list_of_urls.append(a['href'])\n",
    "    except : \n",
    "         print(\"An exception occurred\")\n",
    "    usnews_urls = list(set(list_of_urls))\n",
    "    \n",
    "    return usnews_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "779e2f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_from_url(url):\n",
    "    '''\n",
    "    Description: Retrieving article content from a url and cleaning out the copyright mention\n",
    "    Input: url string of a single article\n",
    "    Output: string with all the article text\n",
    "    Requirements : Requests, BeautifulSoup,Json\n",
    "    Use: This function is made to be used to scrap the to usnews.com website. \n",
    "    If you want to adapt it to another website the copyright sentence should be adapted.\n",
    "    '''\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}\n",
    "    response = requests.get(url,headers=headers) #http request with a user-agent string to avoid blocking from server\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') #parse the document with html format\n",
    "    #find the article in the html page\n",
    "    jsonArticle = json.loads(soup.find(type=\"application/ld+json\").string)\n",
    "    text=jsonArticle['articleBody']\n",
    "    #remove the copyright sentence to avoid it to appear in the most frequent bigrams\n",
    "    clean_text = text.replace('.Copyright 2021 The&nbsp;Associated Press. All rights reserved. This material may not be published, broadcast, rewritten or redistributed.',' ').replace('Associated Press',' ').replace('quot',' ')\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3048bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_articles(usnews_urls):\n",
    "    '''\n",
    "    Description: Loop on a primary list on URL to call the article_from_url function\n",
    "    Input: list of urls strings\n",
    "    Output: one string with all articles contents appended\n",
    "    Requirements : Requests, BeautifulSoup, Json\n",
    "    Use: see article_from_url\n",
    "    '''\n",
    "    all_articles = ''\n",
    "    for url in usnews_urls:\n",
    "        all_articles = all_articles +' '+article_from_url(url)\n",
    "    \n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "675fa14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_frequency(text):\n",
    "    '''\n",
    "    Description: Counting the frequency of n-grams in the text\n",
    "    Input: A single string containing the text of interest \n",
    "    Output: List of bigram and their counts in the text in the format ((string,string),integer)\n",
    "    Requirement: Nltk with stopwords, Counter \n",
    "    Use: this function is set to find bigrams, it can be extended for other n-grams\n",
    "    '''\n",
    "    \n",
    "    #separate the text into words \n",
    "    allWords = nltk.tokenize.word_tokenize(text) \n",
    "    \n",
    "    #gets rid on 1-letter words and 2-letters words\n",
    "    allLongWords = []\n",
    "    for word in allWords:\n",
    "        if len(word) > 2: \n",
    "            allLongWords.append(word)   \n",
    "    #get rid of common english words\n",
    "    stopwords = nltk.corpus.stopwords.words('english') #list of words such as a, the, and etc..\n",
    "    allWordExceptStop =[]\n",
    "    for w in allLongWords:\n",
    "        if w.lower() not in stopwords:\n",
    "            allWordExceptStop.append(w)\n",
    "    #create a list of bigrams words in the text. Can be adapted to n-grams zipping more words\n",
    "    bigrams = zip(allWordExceptStop, allWordExceptStop[1:])\n",
    "    #calculate the frequency of each bigram \n",
    "    bigramsFreq = nltk.FreqDist(bigrams) \n",
    "    return bigramsFreq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51234343",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5c368e",
   "metadata": {},
   "source": [
    "**Usnews.com** has a long list of [topics](https://www.usnews.com/topics/subjects). We decided to focus on political women's rights topics and we've chosen the 5 following links. We tried to run the bigram frequency with women's health and women's history included but too many words related to health or history were coming up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a45f2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of primary links containing articles of interest\n",
    "URL_TOPIC_LIST = ['https://www.usnews.com/topics/subjects/feminism',\n",
    "            'https://www.usnews.com/topics/subjects/gender',\n",
    "            'https://www.usnews.com/topics/subjects/gender_bias',\n",
    "            'https://www.usnews.com/topics/subjects/sexism',\n",
    "            'https://www.usnews.com/topics/subjects/women\\'s rights' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327711e",
   "metadata": {},
   "source": [
    "### Retrieving of articles of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce9bdf0",
   "metadata": {},
   "source": [
    "With the functions defined above we scrape the topic pages for articles references and retrieve the articles contents. The functions deal with different selection steps to avoid all the other contents at each step,i.e. advertisement, galleries, recommended articles etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d82da81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    }
   ],
   "source": [
    "all_articles = ''\n",
    "for url_topic in URL_TOPIC_LIST:\n",
    "    #Retrieve all urls for latest articles in the specific feminism subject page\n",
    "    usnews_topic_urls = get_urls_usnews(url_topic)\n",
    "    \n",
    "    #Retrieve all the articles contents for the latest articles\n",
    "    all_articles_topic =  get_all_articles(usnews_topic_urls)\n",
    "    \n",
    "    #append articles to create one text\n",
    "    all_articles = all_articles +' '+all_articles_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f02fd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MADRID (AP) — A Spanish foundation on Wednesday awarded one of the country’s most prestigious awards to U.S. writer and activist Gloria Steinem.The jury that decides the Princess of Asturias Awards announced that Steinem has won its annual prize for communication and humanities.It praised 87-year-old Steinem’s long career in journalism, her bestselling books and her dedication to feminism since the 1960s, ensuring her place as “one of the most significant and iconic figures of the women’s rights movement” in the United States.The citation singled out her contribution to the legalization of abortion, pay equality and equal rights, as well as her fight against the death penalty, female genital mutilation and child abuse.The 50,000-euro award ($61,000) is one of eight prizes, including in the arts, social sciences and sports, handed out annually by a foundation named for Spanish Crown Princess Leonor  By ERALDO PERES and DIANE JEANTET,  GOIANIA, BRAZIL (AP) — Struck with grief, tens of thousands of fans gathered Saturday to pay tribute to Marília Mendonça, one of Brazil's most popular singers who was killed a day earlier in an airplane crash at age 26.The Latin Grammy winner and four other passengers, including her producer and unc\n"
     ]
    }
   ],
   "source": [
    "print(all_articles[0:1250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "65154a4e-b917-4118-aaac-cd55e578037b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173557"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "27f52251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export all the articles of interest in a single text file\n",
    "text_file = open(\"generated_data/Articles-Contents.txt\", \"w\")\n",
    "text_file.write(all_articles)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c34af",
   "metadata": {},
   "source": [
    "### Frequency computation for bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f1536734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Couting bigram frequencies for all articles of interest\n",
    "usNewsFEMbigramFreq = ngram_frequency(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "da5fc172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Los', 'Angeles');23\n",
      "('gender', 'equality');22\n",
      "('New', 'York');19\n",
      "('Black', 'women');13\n",
      "('child', 'care');13\n",
      "('White', 'House');12\n",
      "('Angeles', 'County');12\n",
      "('health', 'care');11\n",
      "('men', 'pay');11\n",
      "('percentage', 'men');11\n",
      "('Women', 'pay');10\n",
      "('pay', 'percentage');10\n",
      "('Hillary', 'Clinton');8\n",
      "('Best', 'Countries');8\n",
      "('vice', 'president');8\n",
      "('sexual', 'harassment');8\n",
      "('women', 'girls');7\n",
      "('United', 'States');7\n",
      "('girls', 'women');7\n",
      "('Washington', 'D.C.');7\n",
      "('electoral', 'system');7\n",
      "('Donald', 'Trump');7\n",
      "('Supreme', 'Court');6\n",
      "('coronavirus', 'pandemic');6\n",
      "('share', 'women');6\n",
      "('one', 'highest');6\n",
      "('women', 'according');6\n",
      "('rates', 'women');6\n",
      "('Countries', 'rankings');6\n",
      "('Middle', 'East');6\n",
      "('Board', 'Supervisors');6\n",
      "('female', 'mayors');6\n",
      "('young', 'people');5\n",
      "('Iron', 'John');5\n",
      "('York', 'Times');5\n",
      "('first', 'time');5\n",
      "('five', 'years');5\n",
      "('Ford', 'Foundation');5\n",
      "('social', 'media');5\n",
      "('gender', 'stereotypes');5\n",
      "('Hayes', 'said');5\n",
      "('COVID-19', 'crisis');5\n",
      "('COVID-19', 'pandemic');5\n",
      "('sex', 'discrimination');5\n",
      "('public', 'schools');5\n",
      "('states', 'women');5\n",
      "('death', 'rate');5\n",
      "('state', 'budgets');5\n",
      "('gender', 'gap');5\n",
      "('women', 'representation');5\n"
     ]
    }
   ],
   "source": [
    "MAX = 50\n",
    "\n",
    "#Visualize the most common bigrams\n",
    "for word, frequency in usNewsFEMbigramFreq.most_common(MAX):\n",
    "        print('%s;%d' % (word, frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ddf95f",
   "metadata": {},
   "source": [
    "The most common bigrams list also contain a lot of Named Entities (NE) like cities, persons etc... \n",
    "We can see *'Los', 'Angeles'* and *'Donald' 'Trump'* as common bigrams.\n",
    "Here we use the naive approach to ignore this name by using the word capitalization to select them. Note that there are more advanced way to recognize NE (for example Stanforde NER library) but we believe that it will be overkilled for our usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86e0a90",
   "metadata": {},
   "source": [
    "### Final List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2971a46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender equality',\n",
       " 'child care',\n",
       " 'health care',\n",
       " 'men pay',\n",
       " 'percentage men',\n",
       " 'pay percentage',\n",
       " 'vice president',\n",
       " 'sexual harassment',\n",
       " 'women girls',\n",
       " 'girls women',\n",
       " 'electoral system',\n",
       " 'coronavirus pandemic',\n",
       " 'share women',\n",
       " 'one highest',\n",
       " 'women according',\n",
       " 'rates women',\n",
       " 'female mayors',\n",
       " 'young people',\n",
       " 'first time',\n",
       " 'five years',\n",
       " 'social media',\n",
       " 'gender stereotypes',\n",
       " 'sex discrimination',\n",
       " 'public schools',\n",
       " 'states women',\n",
       " 'death rate',\n",
       " 'state budgets',\n",
       " 'gender gap',\n",
       " 'women representation',\n",
       " 'became first',\n",
       " \"women n't\",\n",
       " \"n't matter\",\n",
       " 'lose weight',\n",
       " 'women rights',\n",
       " 'women men',\n",
       " 'two years',\n",
       " 'women movement',\n",
       " 'six years',\n",
       " '100 million',\n",
       " 'gender-based violence',\n",
       " 'women still',\n",
       " 'best states',\n",
       " 'top five',\n",
       " 'top states',\n",
       " 'states plus',\n",
       " 'based gender',\n",
       " 'public school',\n",
       " 'federal government',\n",
       " 'education health',\n",
       " 'proportional electoral',\n",
       " 'female candidates',\n",
       " 'regions say',\n",
       " 'metro area',\n",
       " 'entirely female',\n",
       " 'across country',\n",
       " 'largest cities',\n",
       " 'cities female',\n",
       " 'elected first']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_final_list = []\n",
    "MAX = 100\n",
    "for word, frequency in usNewsFEMbigramFreq.most_common(MAX):\n",
    "    if (word[0][0].isupper()==False and word[1][0].isupper()==False): #ignore the Named Entities\n",
    "        bigram_final_list.append(word[0]+' '+word[1])\n",
    "\n",
    "bigram_final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4721f2",
   "metadata": {},
   "source": [
    "We use this list (in its 150 word long version) to extend our personal list of bigrams. However, maybe because of the corpus size there are still some bigrams that aren't of interest. For example, the *health care* or *vice president* are ignored because the former is too general and the latter irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ce684508",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"women's right\",\n",
       " 'Equal opportunities',\n",
       " 'Equal rights',\n",
       " 'Equal status',\n",
       " 'equal pay',\n",
       " 'gender gap',\n",
       " 'Gender discrimination',\n",
       " 'Gender equality',\n",
       " 'Sexual harrasment',\n",
       " 'Women empowerment',\n",
       " 'women victim',\n",
       " 'women immigration',\n",
       " 'Women emancipation',\n",
       " \"women's participation\",\n",
       " 'Western women',\n",
       " 'non-western woman',\n",
       " 'Muslim women',\n",
       " 'Equal wages',\n",
       " 'Gender equality',\n",
       " 'gender equity',\n",
       " 'Men and women',\n",
       " 'women and men',\n",
       " 'women oppression',\n",
       " 'abortion',\n",
       " 'niqab banstruggle of girls',\n",
       " 'struggle of women',\n",
       " 'war against women',\n",
       " 'oppression of girls',\n",
       " 'oppression of women',\n",
       " 'women oppression',\n",
       " \"women's opression\",\n",
       " 'liberate women',\n",
       " 'religious oppresion',\n",
       " 'abuse of women',\n",
       " 'Male oppression',\n",
       " 'Female oppression',\n",
       " 'Exploitation of women',\n",
       " 'Indigenous women',\n",
       " 'Patriarchal culture',\n",
       " 'gender equality',\n",
       " 'child care',\n",
       " 'men pay',\n",
       " 'percentage men',\n",
       " 'pay percentage',\n",
       " 'sexual harassment',\n",
       " 'women girls',\n",
       " 'girls women',\n",
       " 'rates women',\n",
       " 'women according',\n",
       " 'female mayors',\n",
       " 'share women',\n",
       " 'women movement',\n",
       " 'see women',\n",
       " 'gender stereotypes',\n",
       " 'gender gap',\n",
       " 'women representation',\n",
       " 'sex discrimination',\n",
       " 'women rights',\n",
       " 'woman time',\n",
       " 'based gender',\n",
       " 'female candidates',\n",
       " 'gender-based violence',\n",
       " 'entirely female']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_usnews_keywords = ['gender equality','child care','men pay','percentage men',\n",
    "              'pay percentage','sexual harassment','women girls','girls women',\n",
    "              'rates women','women according','female mayors','share women','women movement',\n",
    "              'see women','gender stereotypes','gender gap', 'women representation','sex discrimination',\n",
    "              'women rights','woman time','based gender', 'female candidates','gender-based violence','entirely female']\n",
    "            \n",
    "#Personal keywords list \n",
    "my_bigrams = ['women\\'s right','Equal opportunities','Equal rights','Equal status',\n",
    "           'equal pay','gender gap','Gender discrimination','Gender equality','Sexual harrasment','Women empowerment',\n",
    "            'women victim','women immigration','Women emancipation','women\\'s participation','Western women','non-western woman',\n",
    "              'Muslim women','Muslim woman', 'Equal wages','Gender equality',\n",
    "             'gender equity','Men and women', 'women and men', 'women oppression','abortion', 'niqab ban'\n",
    "           'struggle of girls','struggle of women', 'war against women','oppression of girls',\n",
    "            'oppression of women','women oppression','women\\'s opression','liberate women','religious oppresion',\n",
    "           'abuse of women','Male oppression','Female oppression','Exploitation of women',\n",
    "           'Indigenous women','Patriarchal culture']\n",
    "\n",
    "all_bigrams = my_bigrams + selected_usnews_keywords\n",
    "all_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc04bf1a",
   "metadata": {},
   "source": [
    "The next step save a keywords text file and recall it. This is done once to save important information but the notebook could ba run directly without the export and import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8658b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export all the keywords in a single text file\n",
    "text_file = open(\"generated_data/Keywords.txt\", \"w\")\n",
    "for bigram in all_bigrams:\n",
    "    text_file.write(bigram+',')\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f3792022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import all the keywords in a single text file\n",
    "KEYWORDS_LIST = [] \n",
    "# opening the text file\n",
    "with open(\"generated_data/Keywords.txt\", \"r\") as file:\n",
    " \n",
    "    # reading each line    \n",
    "    for line in file:\n",
    "   \n",
    "        # reading each word        \n",
    "        for word in line.split(','):\n",
    "   \n",
    "            # displaying the words           \n",
    "            KEYWORDS_LIST.append(word) \n",
    "#KEYWORDS_LIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e3cd3",
   "metadata": {},
   "source": [
    "## Dataset selection from Quotebank database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bceebc3",
   "metadata": {},
   "source": [
    "### Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6b10f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec3a1d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2ade9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing on chunk\n",
    "#Input\n",
    "#Output\n",
    "def process_chunk(chunk, vocabulary):\n",
    "    print(f'Processing chunk with {len(chunk)} rows')\n",
    "    #print(chunk.columns)\n",
    "    occurences = np.zeros(len(vocabulary))\n",
    "    for index, word in enumerate(vocabulary):\n",
    "        occurences[index] = np.sum(chunk['quotation'].str.contains(word)) \n",
    "    return occurences\n",
    "\n",
    "#Select quotes containing keywords\n",
    "def select_quotes_chunk(chunk, keywords):\n",
    "    print(f'Processing chunk with {len(chunk)} rows')\n",
    "    return chunk[chunk['quotation'].str.contains('|'.join(keywords),case=False)]\n",
    "\n",
    "#Use the selection function on each chunk of the full dataset \n",
    "def select_quotes_one_year(path_to_file, vocabulary, chunksize = 10 ** 4):\n",
    "    with pd.read_json(path_to_file, lines=True, compression='bz2', chunksize=chunksize) as df_reader:\n",
    "        for index, chunk in enumerate(df_reader):\n",
    "            if not index==0:\n",
    "                selected_df = pd.concat([selected_df, select_quotes_chunk(chunk, vocabulary)])\n",
    "            else: \n",
    "                selected_df = select_quotes_chunk(chunk, vocabulary)\n",
    "    return selected_df\n",
    "\n",
    "#Use the selection function on each chunk of the full dataset \n",
    "#Dumps the selected quotes into a new json file\n",
    "def select_and_dump(path_to_file, vocabulary, chunksize = 10 ** 4, year = 'replace_me'):\n",
    "    with pd.read_json(path_to_file, lines=True, compression='bz2', chunksize=chunksize) as df_reader:\n",
    "        for index, chunk in enumerate(df_reader):\n",
    "            #Dump selected quotes\n",
    "            selected_df = select_quotes_chunk(chunk, vocabulary)\n",
    "            pickle_file_name = year + '_chunk_' + str(index) + '.pkl'\n",
    "            selected_df.to_pickle('files/'+pickle_file_name)\n",
    "            #if not index==0:\n",
    "                #selected_df = pd.concat([selected_df, select_quotes_chunk(chunk, vocabulary)])\n",
    "            #else: \n",
    "               # selected_df = select_quotes_chunk(chunk, vocabulary)\n",
    "    return selected_df\n",
    "\n",
    "\n",
    "import random, string\n",
    "\n",
    "def randomword(length):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb81c5cd",
   "metadata": {},
   "source": [
    "### Constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ccd03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "QUOTEBANK_2020 = DATA_FOLDER+ \"quotes-2020.json.bz2\"\n",
    "QUOTEBANK_2019 = DATA_FOLDER+ \"quotes-2019.json.bz2\"\n",
    "QUOTEBANK_2017 = DATA_FOLDER+ \"quotes-2017.json.bz2\"\n",
    "QUOTEBANK_2015 = DATA_FOLDER+ \"quotes-2015.json.bz2\"\n",
    "QUOTEBANK_2018 = DATA_FOLDER+ \"quotes-2018.json.bz2\"\n",
    "QUOTEBANK_2016 = DATA_FOLDER+ \"quotes-2016.json.bz2\"\n",
    "\n",
    "PATH = 'generated_data/'\n",
    "\n",
    "PARQUET_FILE = PATH +  \"speaker_attributes.parquet\"\n",
    "\n",
    "KEYWORDS_LIST = ('women\\'s right','Equal opportunities','Equal rights','Equal status','equal pay',\n",
    "              'gender gap','Gender discrimination','Gender equality','Sexual harrassment',\n",
    "              'Women empowerment','women victim','women immigration','Women emancipation',\n",
    "              'women\\'s participation','Western women','non-western woman','Muslim women',\n",
    "              'Equal wages','Gender equality','gender equity','Men and women','women and men',\n",
    "              'women oppression','niqab ban','struggle of girls','struggle of women','war against women',\n",
    "              'oppression of girls','oppression of women','women oppression','women\\'s opression','liberate women',\n",
    "              'religious oppresion','abuse of women','Male oppression','Female oppression','Exploitation of women',\n",
    "              'Indigenous women','Patriarchal culture','gender equality','child care','men pay','percentage men',\n",
    "              'pay percentage','sexual harassment','women girls','girls women',\n",
    "              'rates women','women according','female mayors','share women','women movement',\n",
    "              'see women','gender stereotypes','gender gap',\n",
    "              'women representation','sex discrimination','states women',\n",
    "              'women rights','woman time',\n",
    "              'based gender',\n",
    "              'proportional electoral','female candidates','gender-based violence','entirely female','cities female')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f310584f",
   "metadata": {},
   "source": [
    "### Select and pickle of quotes of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c776a7",
   "metadata": {},
   "source": [
    "Note: This code has to be run once to create the pickle files containing the quotes of interest. For futher use, the dataframe is direcly loaded from the pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4065309",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframesNames = ('QOI_2015_DF','QOI_2016_DF','QOI_2017_DF','QOI_2018_DF','QOI_2019_DF','QOI_2020_DF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cb19861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time QOI_2015_DF = select_quotes_one_year(QUOTEBANK_2015,KEYWORDS_LIST,10 ** 4)\n",
    "# %time QOI_2016_DF = select_quotes_one_year(QUOTEBANK_2016,KEYWORDS_LIST,10 ** 4)\n",
    "# %time QOI_2017_DF = select_quotes_one_year(QUOTEBANK_2017,KEYWORDS_LIST,10 ** 4)\n",
    "# %time QOI_2018_DF = select_quotes_one_year(QUOTEBANK_2018,KEYWORDS_LIST,10 ** 4)\n",
    "# %time QOI_2019_DF = select_quotes_one_year(QUOTEBANK_2019,KEYWORDS_LIST,10 ** 4)\n",
    "# %time QOI_2020_DF = select_quotes_one_year(QUOTEBANK_2020,KEYWORDS_LIST,10 ** 4)\n",
    "\n",
    "#for i in range(len(dataframesNames)):\n",
    "#    dataframes[i].to_pickle('generated_data/'+dataframesNames[i]+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a6ac7b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = 'generated_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a72560a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate into one dataframes the dataframes from each pickle file. \n",
    "df = pd.concat([pd.read_pickle(PATH+ fp +'.pkl') for fp in dataframesNames], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d10f3ef3-30e7-450a-b205-4825e43be1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"keywords\"] = df[\"quotation\"].apply(lambda x : [keyword for keyword in KEYWORDS_LIST if keyword in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ee1a91b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-09-004706</td>\n",
       "      <td>Anything less than women winning 50 per cent o...</td>\n",
       "      <td>Katy Gallagher</td>\n",
       "      <td>[Q463507]</td>\n",
       "      <td>2015-03-09 12:30:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Katy Gallagher, 0.5872], [None, 0.4128]]</td>\n",
       "      <td>[http://www.smh.com.au/act-news/women-need-to-...</td>\n",
       "      <td>E</td>\n",
       "      <td>[gender equality]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-04-24-025718</td>\n",
       "      <td>I'd like to congratulate all the winners and f...</td>\n",
       "      <td>Helena Morrissey</td>\n",
       "      <td>[Q23762081]</td>\n",
       "      <td>2015-04-24 15:33:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Helena Morrissey, 0.8706], [None, 0.1294]]</td>\n",
       "      <td>[http://www.cipd.co.uk/PM/peoplemanagement/b/w...</td>\n",
       "      <td>E</td>\n",
       "      <td>[gender equality]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-07-16-044620</td>\n",
       "      <td>I think what Deepika has spoken in the video m...</td>\n",
       "      <td>Kalki Koechlin</td>\n",
       "      <td>[Q3192216]</td>\n",
       "      <td>2015-07-16 16:41:07</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Kalki Koechlin, 0.6377], [None, 0.3623]]</td>\n",
       "      <td>[http://www.pinkvilla.com/entertainmenttags/ka...</td>\n",
       "      <td>E</td>\n",
       "      <td>[Women empowerment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-09-11-052815</td>\n",
       "      <td>if advocating for equal pay for equal work is ...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>[Q6294]</td>\n",
       "      <td>2015-09-11 14:17:08</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Hillary Clinton, 0.8831], [None, 0.1105], [D...</td>\n",
       "      <td>[http://www.wrn.com/2015/09/hillary-clinton-ra...</td>\n",
       "      <td>E</td>\n",
       "      <td>[equal pay]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-04-23-037713</td>\n",
       "      <td>Men and women are understandably upset if they...</td>\n",
       "      <td>Jim McDermott</td>\n",
       "      <td>[Q321457, Q6196778]</td>\n",
       "      <td>2015-04-23 21:52:22</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Jim McDermott, 0.629], [John F. Kerry, 0.190...</td>\n",
       "      <td>[http://www.atlanticcouncil.org/en/blogs/new-a...</td>\n",
       "      <td>E</td>\n",
       "      <td>[Men and women]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID                                          quotation  \\\n",
       "0  2015-03-09-004706  Anything less than women winning 50 per cent o...   \n",
       "1  2015-04-24-025718  I'd like to congratulate all the winners and f...   \n",
       "2  2015-07-16-044620  I think what Deepika has spoken in the video m...   \n",
       "3  2015-09-11-052815  if advocating for equal pay for equal work is ...   \n",
       "4  2015-04-23-037713  Men and women are understandably upset if they...   \n",
       "\n",
       "            speaker                 qids                date  numOccurrences  \\\n",
       "0    Katy Gallagher            [Q463507] 2015-03-09 12:30:00               1   \n",
       "1  Helena Morrissey          [Q23762081] 2015-04-24 15:33:00               1   \n",
       "2    Kalki Koechlin           [Q3192216] 2015-07-16 16:41:07               1   \n",
       "3   Hillary Clinton              [Q6294] 2015-09-11 14:17:08               1   \n",
       "4     Jim McDermott  [Q321457, Q6196778] 2015-04-23 21:52:22               1   \n",
       "\n",
       "                                              probas  \\\n",
       "0         [[Katy Gallagher, 0.5872], [None, 0.4128]]   \n",
       "1       [[Helena Morrissey, 0.8706], [None, 0.1294]]   \n",
       "2         [[Kalki Koechlin, 0.6377], [None, 0.3623]]   \n",
       "3  [[Hillary Clinton, 0.8831], [None, 0.1105], [D...   \n",
       "4  [[Jim McDermott, 0.629], [John F. Kerry, 0.190...   \n",
       "\n",
       "                                                urls phase  \\\n",
       "0  [http://www.smh.com.au/act-news/women-need-to-...     E   \n",
       "1  [http://www.cipd.co.uk/PM/peoplemanagement/b/w...     E   \n",
       "2  [http://www.pinkvilla.com/entertainmenttags/ka...     E   \n",
       "3  [http://www.wrn.com/2015/09/hillary-clinton-ra...     E   \n",
       "4  [http://www.atlanticcouncil.org/en/blogs/new-a...     E   \n",
       "\n",
       "              keywords  \n",
       "0    [gender equality]  \n",
       "1    [gender equality]  \n",
       "2  [Women empowerment]  \n",
       "3          [equal pay]  \n",
       "4      [Men and women]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eb692941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe has 87161 entries\n"
     ]
    }
   ],
   "source": [
    "print(f'The dataframe has {len(df)} entries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3969377b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Anything less than women winning 50 per cent of new seats will be a loss not only for a progressive city's progress towards true gender equality but it would also be a loss for good governance in ...\n",
       "1    I'd like to congratulate all the winners and finalists on their success. They have demonstrated clear leadership by moving women's progression from a `diversity' initiative to a core business prio...\n",
       "2    I think what Deepika has spoken in the video makes sense. I do understand the counter argument too where everyone has been saying that had men said the same lines about having sex outside marriage...\n",
       "3                                                                                    if advocating for equal pay for equal work is playing the gender card, deal me in. I am ready to play as hard as I can.\n",
       "4      Men and women are understandably upset if they see a company close down and jobs lost. It's only natural people would look around and in their distress they find something or someone able to blame,\n",
       "Name: quotation, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "df.head()['quotation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b872efd",
   "metadata": {},
   "source": [
    "Since the dates in the quotes don't seem to be a problem, and our current method to parse dates sometimes give a memory overflow, we currently won't be removing them but will remove only the html "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
